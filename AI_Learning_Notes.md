# 人工智慧與深度學習應用
---
## W2
### 何謂人工智慧?
- 是研究、開發用於模擬、延伸和擴展人的智能的理論、方法、技術及應用系統的一門新的技術科學。
### 智能的定義
- 唯一種推理能力、學習、理解、處理
### AI於1943年誕生，以神經元為基礎建構的數學模型
- 藉圖靈實驗，如果通過則稱為智慧的，實驗本質為:讓人在不知情的情況下，不能分別自己是跟機器還是人互動
- 1980 年， 「機器學習」（Machine Learning）開始蓬勃興起，歸功於硬體儲存成本下降、運算能力增強，大量數據即時處理。
- 1986 年，Hinton 等人提出反向傳播法（Backpropagation），降低了類神經網路（Neural Network） 的計算量， 一度興起類神經網路，但反向傳播仍受到梯度消失的問題（vanishing gradientproblem），漸漸的凋零。進而轉向淺層深度學習- 支持向量機（SVM， Support Vector Machine）興起。

### 人工智慧
- 強人工智慧
    - 可能製造出真正能推理和解決問題之智能機器人(有知覺)
    - 2種
        - 類人
            - 機器思維即人的思維
        - 非類人
            - 即機器產生了和人完全不一樣的知覺和意識，使用和人完全不一樣的推理方式
- 弱人工智慧 
    - 弱人工智慧觀點認為不可能製造出能真正地推理和解決問題的智能機器，這些機器只不過看起來像是智能的，但是並不真正擁有智能，也不會有自主意識。
### 監督式學習、非監督式學習、增強學習
- 監督式學習
    - 給定訓練資料集中學習出的一個函式，新的資料傳入時根據此來預測及判斷
    - 監督學習和非監督學習的差別就是訓練集目標是否由人標註
### AI包含的範疇
- 機器學習
    - 大數據分散式儲存
        - 監督式學習
        - 非監督式學習
        - 增強式學習
- 深度學習 
    - GPU、TPU 平行運算
        - DNN
        - CNN
        - RNN 
### 機器學習 vs 圖形辨識


| Machine Learning | Pattern Recognition |
| -------- | -------- | 
| 機器學習是一種自動化分析模型建構的數據分析方法| 圖型辨識是各種演算法的工程應用，目的是識別數據中的模式|
|機器學習著重在實務應用上 |圖型識別是著重在理論方面|

### 深度學習
- 深度學習（deep learning）是機器學習的另一分支，以類神經網路為架構，對資料進行表徵(或特徵 feature) 學習的演算法。
- 常見的架構有深度神經網路、卷積神經網路(CNN)和深度置信網路(DBN)和遞迴神經網路(RNN)

---
## W5
### 何謂感知器?
- 感知器(perceptron), 又稱作為人工神經元(artificial neural)或單純感知器(simpleperceptron).
- 感知器是收到多個訊號之後, 再當作一個訊號的輸出
- 權重(weight)是控制個訊號重要的元素, 換句話說, 權重愈重,對應該權重的訊號就愈重要
- 單層感知器無法表現的部分(非線性區域), 可以利用層疊感知器的方式(加深層數)來描述非線性的部分
- 活化函式:
    - 輸入訊號的總和轉換成輸出訊號的函式, 活化的意思, 顧名思義就是活化函式具有決定如何活化輸入訊號總和的功能
- sigmoid function是平滑曲線(連續性), step function是離散曲線, 平滑度在類神經網路中有重要的意義
- 以水流來說, 0與1只有通過與不通過, sigmoid function 來看是可以控制通過水量的比例.
- 現今最常用的活化函式, 是ReLU函式(Recitified Linear Unit function), 輸出超過0, 就會將輸入直接輸出, 如果是0以下, 就輸出0的函式.
- 什麼是"最佳"函式 ?
    - "最佳"函式 等於 "最佳"參數集
    - 因為不一樣的參數(W and b)決定不一樣的函式
    - 選擇"最佳"函式" 等於選擇 "最佳"參數集
- 成本函式(Cost function)
    - 也可以稱為損失函式(loss function), 錯誤函式(error function)或目標函式(objective function)
    - 常見的損失函式(loss function)
    - 均方誤差 (mean squared error)
    - 交叉熵誤差(cross entropy error)
## W6
### 梯度法
- 損失函式很複雜, 參數空間很大, 無法預測哪裡才是取得最小值的位置. 因此善用梯度找出最小值, 稱為梯度法

### 梯度消失
- 梯度消失問題（Vanishing gradient problem）是一種機器學習中的難題，出現在以梯度下降法和反向傳播訓練人工神經網絡的時候。 在每次訓練的疊代中，神經網絡權重的更新值與誤差函數的偏導數成比例，然而在某些情況下，梯度值會幾乎消失，使得權重無法得到有效更新，甚至神經網絡可能完全無法繼續訓練。
### 為什麼需要有Batch_Size?
- batchsize的正確選擇是為了在記憶體效率和記憶體容量之間尋找最佳平衡
### 適當的增加Batch_Size的優點：
- 1.通過並行化提高記憶體利用率。
- 2.單次epoch的迭代次數減少，提高執行速度。（單次epoch=(全部訓練樣本/batchsize)/iteration=1）
- 3.適當的增加Batch_Size,梯度下降方向準確度增加，訓練震動的幅度減小
#### batchsize：
- 批大小，在深度學習中，一般採用SGD訓練，即每次訓練在訓練集中取batchsize個樣本訓練
#### iteration：
- 1個iteration等於使用batchsize個樣本訓練一次
#### epoch：
- 1個epoch等於使用訓練集中的全部樣本訓練一次
[以上三項參考](https://codertw.com/%E7%A8%8B%E5%BC%8F%E8%AA%9E%E8%A8%80/557816/)
## W7
## W8
---
### Batch Normalization
- 可以快速學習 (增加學習率)
- 不會過度依賴預設值
- 控制過度學習 (減少Dropout等必要性)
### 值得參考:[梯度下降法](https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E6%95%B8%E5%AD%B8-%E4%BA%8C-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-gradient-descent-406e1fd001f)
### 值得參考:[梯度最佳解相關法](https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E6%95%B8%E5%AD%B8-%E4%B8%89-%E6%A2%AF%E5%BA%A6%E6%9C%80%E4%BD%B3%E8%A7%A3%E7%9B%B8%E9%97%9C%E7%AE%97%E6%B3%95-gradient-descent-optimization-algorithms-b61ed1478bd7)

